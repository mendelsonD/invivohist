{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jordan shared. For stitching together hippunfold + fsLRsurfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.gifti import GiftiImage, GiftiDataArray\n",
    "from nibabel.nifti1 import intent_codes\n",
    "\n",
    "def gifti_remove_bad_vertices(gi: GiftiImage, bad_idx) -> GiftiImage:\n",
    "    \"\"\"\n",
    "    Remove vertices and any faces that reference them from a GiftiImage.\n",
    "    Also updates per-vertex and per-face data arrays accordingly.\n",
    "    \"\"\"\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "\n",
    "    pointset_arrs = [da for da in gi.darrays if da.intent == POINTSET]\n",
    "    tri_arrs = [da for da in gi.darrays if da.intent == TRIANGLE]\n",
    "    if len(pointset_arrs) != 1 or len(tri_arrs) != 1:\n",
    "        raise ValueError(\"GiftiImage must contain exactly one POINTSET and one TRIANGLE data array.\")\n",
    "\n",
    "    verts_da = pointset_arrs[0]\n",
    "    faces_da = tri_arrs[0]\n",
    "\n",
    "    V = np.asarray(verts_da.data)   # (N, 3)\n",
    "    F = np.asarray(faces_da.data)   # (M, 3)\n",
    "    N, M = V.shape[0], F.shape[0]\n",
    "\n",
    "    if bad_idx.size == 0:\n",
    "        return gi\n",
    "\n",
    "    vmask = np.ones(N, dtype=bool)\n",
    "    vmask[bad_idx] = False\n",
    "    if not np.any(vmask):\n",
    "        raise ValueError(\"All vertices would be removed.\")\n",
    "\n",
    "    new_index = -np.ones(N, dtype=np.int64)\n",
    "    new_index[vmask] = np.arange(vmask.sum())\n",
    "\n",
    "    fmask = np.all(vmask[F], axis=1)\n",
    "    V_new = V[vmask]\n",
    "    F_new = new_index[F[fmask]].astype(np.int32)\n",
    "\n",
    "    def _remap_da(da: GiftiDataArray) -> GiftiDataArray:\n",
    "        data = np.asarray(da.data)\n",
    "        if da.intent not in (POINTSET, TRIANGLE):\n",
    "            if data.shape[0] == N:\n",
    "                data = data[vmask]\n",
    "            elif data.shape[0] == M:\n",
    "                data = data[fmask]\n",
    "        return GiftiDataArray(\n",
    "            data=data,\n",
    "            intent=da.intent,\n",
    "            datatype=da.datatype,\n",
    "            encoding=da.encoding,\n",
    "            endian=da.endian,\n",
    "            coordsys=da.coordsys,\n",
    "            meta=da.meta,\n",
    "        )\n",
    "\n",
    "    new_gi = GiftiImage(meta=gi.meta, labeltable=gi.labeltable)\n",
    "    for da in gi.darrays:\n",
    "        if da.intent == POINTSET:\n",
    "            da_out = GiftiDataArray(\n",
    "                data=V_new,\n",
    "                intent=POINTSET,\n",
    "                datatype=da.datatype,\n",
    "                encoding=da.encoding,\n",
    "                endian=da.endian,\n",
    "                coordsys=da.coordsys,\n",
    "                meta=da.meta,\n",
    "            )\n",
    "        elif da.intent == TRIANGLE:\n",
    "            da_out = GiftiDataArray(\n",
    "                data=F_new,\n",
    "                intent=TRIANGLE,\n",
    "                datatype=da.datatype,\n",
    "                encoding=da.encoding,\n",
    "                endian=da.endian,\n",
    "                coordsys=da.coordsys,\n",
    "                meta=da.meta,\n",
    "            )\n",
    "        else:\n",
    "            da_out = _remap_da(da)\n",
    "        new_gi.add_gifti_data_array(da_out)\n",
    "\n",
    "    return new_gi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.nifti1 import intent_codes\n",
    "from nibabel.gifti import GiftiImage\n",
    "\n",
    "def _get_points(x):\n",
    "    \"\"\"Extract (N,3) POINTSET array from a GiftiImage or pass-through if already ndarray.\"\"\"\n",
    "    if isinstance(x, GiftiImage):\n",
    "        POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "        arrs = [da for da in x.darrays if da.intent == POINTSET]\n",
    "        if len(arrs) != 1:\n",
    "            raise ValueError(\"GiftiImage must contain exactly one POINTSET data array.\")\n",
    "        pts = np.asarray(arrs[0].data)\n",
    "    else:\n",
    "        pts = np.asarray(x)\n",
    "    if pts.ndim != 2 or pts.shape[1] != 3:\n",
    "        raise ValueError(\"Vertices must be an array of shape (N, 3).\")\n",
    "    return pts\n",
    "\n",
    "def vertices_within_threshold(\n",
    "    surf_a,\n",
    "    surf_b,\n",
    "    threshold_mm: float = 0.5,\n",
    "    return_mask: bool = False,\n",
    "    return_distances: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return indices (or boolean mask) where the vertex-wise Euclidean distance\n",
    "    between two corresponding surfaces is < threshold_mm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    surf_a, surf_b : GiftiImage or np.ndarray\n",
    "        Each either a nibabel GiftiImage (with a single POINTSET) or an (N,3) array.\n",
    "        Assumes 1:1 vertex correspondence and same coordinate space/units.\n",
    "    threshold_mm : float\n",
    "        Distance threshold in mm. Default 0.5.\n",
    "    return_mask : bool\n",
    "        If True, return a boolean mask of shape (N,) instead of indices.\n",
    "    return_distances : bool\n",
    "        If True, also return the per-vertex distances (shape (N,)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idx_or_mask : np.ndarray\n",
    "        Indices where distance < threshold_mm, or boolean mask if return_mask=True.\n",
    "    distances (optional) : np.ndarray\n",
    "        Per-vertex distances (sqrt of squared differences), if return_distances=True.\n",
    "    \"\"\"\n",
    "    A = _get_points(surf_a)\n",
    "    B = _get_points(surf_b)\n",
    "\n",
    "    if A.shape != B.shape:\n",
    "        raise ValueError(f\"Shape mismatch: {A.shape} vs {B.shape}\")\n",
    "\n",
    "    # Handle NaNs/infs robustly: mark them as not-close\n",
    "    valid = np.all(np.isfinite(A), axis=1) & np.all(np.isfinite(B), axis=1)\n",
    "    d2 = np.full(A.shape[0], np.inf, dtype=A.dtype)\n",
    "    d2[valid] = np.sum((A[valid] - B[valid])**2, axis=1)\n",
    "    dist = np.sqrt(d2)\n",
    "\n",
    "    mask = dist < float(threshold_mm)\n",
    "    out = mask if return_mask else np.nonzero(mask)[0]\n",
    "\n",
    "    if return_distances:\n",
    "        return out, dist\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1646\n"
     ]
    }
   ],
   "source": [
    "lbl = nib.load(\"/data/mica1/01_programs/micapipe-v0.2.0/parcellations/schaefer-100_conte69_lh.label.gii\")\n",
    "white = nib.load(\"refsurfs/avg_L_fs_white.surf.gii\")\n",
    "pial = nib.load(\"refsurfs/avg_L_fs_pial.surf.gii\")\n",
    "idx = vertices_within_threshold(white, pial, threshold_mm=0.5)\n",
    "print(len(idx))\n",
    "\n",
    "white_nomed = gifti_remove_bad_vertices(white, idx)\n",
    "# nib.save(white_nomed, \"refsurfs/avg_L_fs_white_nomed.surf.gii\")\n",
    "pial_nomed = gifti_remove_bad_vertices(pial, idx)\n",
    "# nib.save(pial_nomed, \"refsurfs/avg_L_fs_pial_nomed.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nibabel.gifti import GiftiImage\n",
    "from nibabel.nifti1 import intent_codes\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import deque\n",
    "\n",
    "def _gifti_vertices_faces(gi: GiftiImage):\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "    vs = [da for da in gi.darrays if da.intent == POINTSET]\n",
    "    fs = [da for da in gi.darrays if da.intent == TRIANGLE]\n",
    "    if len(vs) != 1 or len(fs) != 1:\n",
    "        raise ValueError(\"GiftiImage must contain exactly one POINTSET and one TRIANGLE array.\")\n",
    "    V = np.asarray(vs[0].data)\n",
    "    F = np.asarray(fs[0].data, dtype=np.int64)\n",
    "    return V, F\n",
    "\n",
    "def _build_vertex_adjacency(faces, n_vertices):\n",
    "    adj = [[] for _ in range(n_vertices)]\n",
    "    for a, b, c in faces:\n",
    "        adj[a].extend([b, c]); adj[b].extend([a, c]); adj[c].extend([a, b])\n",
    "    return [np.unique(nei) for nei in adj]\n",
    "\n",
    "def _largest_components(mask, faces, k=1, min_size=0):\n",
    "    \"\"\"Keep the k largest connected components inside mask (by vertex adjacency).\"\"\"\n",
    "    n = len(mask)\n",
    "    adj = _build_vertex_adjacency(faces, n)\n",
    "    seen = np.zeros(n, dtype=bool)\n",
    "    comps = []\n",
    "    for i in np.where(mask)[0]:\n",
    "        if seen[i]:\n",
    "            continue\n",
    "        q = deque([int(i)])\n",
    "        seen[i] = True\n",
    "        comp = [int(i)]\n",
    "        while q:\n",
    "            u = q.popleft()\n",
    "            for v in adj[u]:\n",
    "                v = int(v)\n",
    "                if not seen[v] and mask[v]:\n",
    "                    seen[v] = True\n",
    "                    q.append(v)\n",
    "                    comp.append(v)\n",
    "        comps.append(np.array(comp, dtype=int))\n",
    "\n",
    "    if not comps:\n",
    "        return np.zeros_like(mask, dtype=bool)\n",
    "\n",
    "    if k is None:\n",
    "        comps = [c for c in comps if c.size >= int(min_size)]\n",
    "    else:\n",
    "        comps = sorted(comps, key=lambda c: c.size, reverse=True)[:int(k)]\n",
    "\n",
    "    out = np.zeros_like(mask, dtype=bool)\n",
    "    for c in comps:\n",
    "        out[c] = True\n",
    "    return out\n",
    "\n",
    "def _morph_mesh(mask: np.ndarray, adj, n_dilate=0, n_erode=0):\n",
    "    \"\"\"Dilate then erode a boolean mask on mesh adjacency graph.\"\"\"\n",
    "    out = mask.copy()\n",
    "    for _ in range(n_dilate):\n",
    "        new_out = out.copy()\n",
    "        for i, neis in enumerate(adj):\n",
    "            if not out[i] and np.any(out[neis]):\n",
    "                new_out[i] = True\n",
    "        out = new_out\n",
    "    for _ in range(n_erode):\n",
    "        new_out = out.copy()\n",
    "        for i, neis in enumerate(adj):\n",
    "            if out[i] and not np.all(out[neis]):\n",
    "                new_out[i] = False\n",
    "        out = new_out\n",
    "    return out\n",
    "\n",
    "def carve_neocortex_by_distance_gifti(\n",
    "    H_gifti: GiftiImage,\n",
    "    C_gifti: GiftiImage,\n",
    "    tau_mm: float = 3.0,\n",
    "    n_dilate: int = 3,\n",
    "    n_erode: int = 5,\n",
    "    keep_largest_component: bool = True,\n",
    "    min_component_size: int = 50,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return indices of neocortical vertices to remove based on distance to hippocampus,\n",
    "    then filter components and apply dilation→erosion morphology.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    H_gifti : GiftiImage\n",
    "        Hippocampal surface.\n",
    "    C_gifti : GiftiImage\n",
    "        Neocortical surface.\n",
    "    tau_mm : float\n",
    "        Distance threshold in mm for removal.\n",
    "    n_dilate : int\n",
    "        Number of dilation iterations.\n",
    "    n_erode : int\n",
    "        Number of erosion iterations (applied after dilation).\n",
    "    keep_largest_component : bool\n",
    "        If True, keep only the largest connected component of the removal mask.\n",
    "    min_component_size : int\n",
    "        Minimum size (in vertices) of a component to keep if not using keep_largest_component.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    remove_idx : np.ndarray\n",
    "        Indices of vertices to remove.\n",
    "    \"\"\"\n",
    "    H_V, _ = _gifti_vertices_faces(H_gifti)\n",
    "    C_V, C_F = _gifti_vertices_faces(C_gifti)\n",
    "\n",
    "    # Distance from each C vertex to nearest H vertex\n",
    "    H_tree = cKDTree(H_V)\n",
    "    dEuc, _ = H_tree.query(C_V, k=1)\n",
    "    remove_mask = dEuc < float(tau_mm)\n",
    "\n",
    "    # Connected component filtering\n",
    "    if keep_largest_component:\n",
    "        remove_mask = _largest_components(remove_mask, C_F, k=1)\n",
    "    elif min_component_size > 0:\n",
    "        remove_mask = _largest_components(remove_mask, C_F, k=None, min_size=min_component_size)\n",
    "\n",
    "    # Morphology: dilate then erode\n",
    "    adj = _build_vertex_adjacency(C_F, len(C_V))\n",
    "    remove_mask = _morph_mesh(remove_mask, adj, n_dilate=n_dilate, n_erode=n_erode)\n",
    "\n",
    "    return np.nonzero(remove_mask)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    }
   ],
   "source": [
    "hipp_o = nib.load(\"refsurfs/avg_L_hu_outer.surf.gii\")\n",
    "hipp_i = nib.load(\"refsurfs/avg_L_hu_inner.surf.gii\")\n",
    "idx = carve_neocortex_by_distance_gifti(hipp_o,white_nomed)\n",
    "print(len(idx))\n",
    "\n",
    "white_nohipp = gifti_remove_bad_vertices(white_nomed, idx)\n",
    "nib.save(white_nohipp, \"refsurfs/avg_L_fs_white_nohipp.surf.gii\")\n",
    "pial_nohipp = gifti_remove_bad_vertices(pial_nomed, idx)\n",
    "nib.save(pial_nohipp, \"refsurfs/avg_L_fs_pial_nohipp.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -39.765625 -199.76562     1.171875]\n",
      "[  -0.234375 -180.23438     1.171875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfold = nib.load(\"/data/mica3/BIDS_PNI/derivatives/hippunfold_v1.3.0/hippunfold/sub-PNC001/ses-01/surf/sub-PNC001_ses-01_hemi-L_space-unfold_den-0p5mm_label-hipp_midthickness.surf.gii\")\n",
    "unfold_V = np.asarray([da.data for da in unfold.darrays if da.intent == intent_codes['NIFTI_INTENT_POINTSET']][0])\n",
    "print(np.min(unfold_V,axis=0))\n",
    "print(np.max(unfold_V,axis=0))\n",
    "bridgehead = np.where(unfold_V[:,1] < -199.7)[0]\n",
    "len(bridgehead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.spatial import cKDTree\n",
    "import nibabel as nib\n",
    "from nibabel.gifti import GiftiImage, GiftiDataArray\n",
    "from nibabel.nifti1 import intent_codes\n",
    "\n",
    "# ---------- basic helpers ----------\n",
    "\n",
    "def _gifti_vertices_faces(gi: GiftiImage):\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "    vs = [da for da in gi.darrays if da.intent == POINTSET]\n",
    "    fs = [da for da in gi.darrays if da.intent == TRIANGLE]\n",
    "    if len(vs) != 1 or len(fs) != 1:\n",
    "        raise ValueError(\"Each GiftiImage must contain exactly one POINTSET and one TRIANGLE.\")\n",
    "    V = np.asarray(vs[0].data)\n",
    "    F = np.asarray(fs[0].data, dtype=np.int64)\n",
    "    if V.ndim != 2 or V.shape[1] != 3 or F.ndim != 2 or F.shape[1] != 3:\n",
    "        raise ValueError(\"Expected V:(N,3) and F:(M,3).\")\n",
    "    return V, F\n",
    "\n",
    "def _boundary_edges_and_vertices(faces, n_vertices):\n",
    "    edge_count = defaultdict(int)\n",
    "    for a,b,c in faces:\n",
    "        for u,v in ((a,b),(b,c),(c,a)):\n",
    "            uv = (min(u,v), max(u,v))\n",
    "            edge_count[uv] += 1\n",
    "    edges_bdry = [uv for uv,cnt in edge_count.items() if cnt == 1]\n",
    "    vert_mask = np.zeros(n_vertices, dtype=bool)\n",
    "    for u,v in edges_bdry:\n",
    "        vert_mask[u] = True; vert_mask[v] = True\n",
    "    badj = defaultdict(list)\n",
    "    for u,v in edges_bdry:\n",
    "        badj[u].append(v); badj[v].append(u)\n",
    "    return edges_bdry, vert_mask, badj\n",
    "\n",
    "def _order_boundary_loop(badj, comp_vertices):\n",
    "    # order a single boundary component (loop or open chain)\n",
    "    deg = {v: len(badj[v]) for v in comp_vertices}\n",
    "    endpoints = [v for v in comp_vertices if deg[v] == 1]\n",
    "    start = endpoints[0] if endpoints else int(comp_vertices[0])\n",
    "    ordered = [start]; prev = None; cur = start\n",
    "    seen = {start}\n",
    "    while True:\n",
    "        nbrs = badj[cur]\n",
    "        nxt = None\n",
    "        for nb in nbrs:\n",
    "            if nb != prev:\n",
    "                nxt = nb; break\n",
    "        if nxt is None or nxt in seen:\n",
    "            break\n",
    "        ordered.append(nxt); seen.add(nxt)\n",
    "        prev, cur = cur, nxt\n",
    "    return np.array(ordered, dtype=int)\n",
    "\n",
    "def _minimal_cover_arc(seed_positions, L):\n",
    "    p = np.sort(np.unique(seed_positions))\n",
    "    if p.size == 1:\n",
    "        s = int(p[0]); return s, s\n",
    "    gaps = np.diff(np.r_[p, p[0] + L])\n",
    "    i_largest = int(np.argmax(gaps))\n",
    "    start = int(p[(i_largest + 1) % p.size])\n",
    "    end   = int(p[i_largest])\n",
    "    return start, end  # circular indices in [0..L-1]\n",
    "\n",
    "def _ring_interval_indices(start, end, L):\n",
    "    return np.arange(start, end+1, dtype=int) if start <= end else \\\n",
    "           np.r_[np.arange(start, L, dtype=int), np.arange(0, end+1, dtype=int)]\n",
    "\n",
    "def _ring_dilate(mask_ring, steps=0):\n",
    "    if steps <= 0: return mask_ring\n",
    "    out = mask_ring.copy()\n",
    "    for _ in range(steps):\n",
    "        out = out | np.roll(out, 1) | np.roll(out, -1)\n",
    "    return out\n",
    "\n",
    "# ---------- DP seam (unchanged) ----------\n",
    "\n",
    "def _dp_seam_triangles(H_xyz, C_xyz, H_idx, C_idx):\n",
    "    \"\"\"\n",
    "    Build a monotone path from (0,0) to (nH-1,nC-1) using only horizontal/vertical steps.\n",
    "    Cost at (i,j) = ||H[i]-C[j]||. Emit one triangle per step:\n",
    "      - horizontal: (H[i-1], H[i],   C[j])\n",
    "      - vertical:   (H[i],   C[j-1], C[j])\n",
    "    Returns tagged triangles: [('H',h0), ('H',h1), ('C',c0)] etc.\n",
    "    \"\"\"\n",
    "    nH, nC = len(H_idx), len(C_idx)\n",
    "    if nH < 2 or nC < 2:\n",
    "        return []\n",
    "\n",
    "    H_pts = H_xyz[H_idx]; C_pts = C_xyz[C_idx]\n",
    "    D = np.linalg.norm(H_pts[:,None,:] - C_pts[None,:,:], axis=2)\n",
    "\n",
    "    acc = np.empty((nH, nC), dtype=float)\n",
    "    acc[0,0] = D[0,0]\n",
    "    for i in range(1, nH): acc[i,0] = acc[i-1,0] + D[i,0]\n",
    "    for j in range(1, nC): acc[0,j] = acc[0,j-1] + D[0,j]\n",
    "    for i in range(1, nH):\n",
    "        for j in range(1, nC):\n",
    "            acc[i,j] = D[i,j] + min(acc[i-1,j], acc[i,j-1])\n",
    "\n",
    "    # backtrack\n",
    "    i, j = nH-1, nC-1\n",
    "    path = [(i,j)]\n",
    "    while i>0 or j>0:\n",
    "        if i==0: j -= 1\n",
    "        elif j==0: i -= 1\n",
    "        elif acc[i-1,j] <= acc[i,j-1]: i -= 1\n",
    "        else: j -= 1\n",
    "        path.append((i,j))\n",
    "    path = path[::-1]\n",
    "\n",
    "    tris = []\n",
    "    for (ia,ja), (ib,jb) in zip(path[:-1], path[1:]):\n",
    "        if ib == ia + 1 and jb == ja:      # horizontal\n",
    "            tris.append((('H', H_idx[ia]), ('H', H_idx[ib]), ('C', C_idx[jb])))\n",
    "        elif jb == ja + 1 and ib == ia:    # vertical\n",
    "            tris.append((('H', H_idx[ib]), ('C', C_idx[ja]), ('C', C_idx[jb])))\n",
    "    return tris\n",
    "\n",
    "# ---------- main (re-ordered by cortical ring coordinate) ----------\n",
    "\n",
    "def stitch_hippocampus_neocortex_gifti_nearestDP(\n",
    "    H_gifti: GiftiImage,\n",
    "    C_gifti: GiftiImage,\n",
    "    H_bridge_idx,\n",
    "    r_nn: float = 8.0,\n",
    "    ring_pad_steps: int = 2\n",
    ") -> GiftiImage:\n",
    "    \"\"\"\n",
    "    Stitch hippocampus ↔ neocortex using a DP seam, with BOTH chains ordered by\n",
    "    the SAME cortical ring coordinate to prevent first↔last crossings.\n",
    "    \"\"\"\n",
    "    H_V, H_F = _gifti_vertices_faces(H_gifti)\n",
    "    C_V, C_F = _gifti_vertices_faces(C_gifti)\n",
    "\n",
    "    # 1) Cortical boundary ring (ordered)\n",
    "    _, C_edge_mask, C_badj = _boundary_edges_and_vertices(C_F, len(C_V))\n",
    "    C_edge_idx = np.where(C_edge_mask)[0]\n",
    "    if C_edge_idx.size < 2:\n",
    "        raise RuntimeError(\"Cortical boundary too small to stitch.\")\n",
    "    C_loop_order = _order_boundary_loop(C_badj, C_edge_idx)  # (L,)\n",
    "    L = len(C_loop_order)\n",
    "\n",
    "    # 2) Map all H bridgeheads to nearest cortical ring position\n",
    "    H_bridge_idx = np.asarray(H_bridge_idx, dtype=int)\n",
    "    H_bridge_xyz = H_V[H_bridge_idx]\n",
    "    ring_tree = cKDTree(C_V[C_loop_order])\n",
    "    d, j = ring_tree.query(H_bridge_xyz, k=1, distance_upper_bound=float(r_nn))\n",
    "    ok = (~np.isinf(d)) & (j < L)\n",
    "    if not np.any(ok):\n",
    "        raise RuntimeError(\"No cortical boundary matches found within r_nn.\")\n",
    "    seed_ring_pos = np.sort(np.unique(j[ok]))  # positions along ring [0..L-1]\n",
    "\n",
    "    # 3) Build a SINGLE contiguous cortical arc: minimal cover of seeds, then pad, then re-make contiguous\n",
    "    s0, e0 = _minimal_cover_arc(seed_ring_pos, L)\n",
    "    base_arc = _ring_interval_indices(s0, e0, L)              # contiguous\n",
    "    ring_mask = np.zeros(L, dtype=bool); ring_mask[base_arc] = True\n",
    "    ring_mask = _ring_dilate(ring_mask, steps=ring_pad_steps) if ring_pad_steps > 0 else ring_mask\n",
    "    # re-make contiguous after dilation (important!)\n",
    "    padded_pos = np.where(ring_mask)[0]\n",
    "    s1, e1 = _minimal_cover_arc(padded_pos, L)\n",
    "    arc_pos_final = _ring_interval_indices(s1, e1, L)         # contiguous, ordered\n",
    "    C_chain = C_loop_order[arc_pos_final]                     # cortical open chain\n",
    "\n",
    "    if C_chain.size < 2:\n",
    "        raise RuntimeError(\"Selected cortical arc too small after padding.\")\n",
    "\n",
    "    # 4) Order the HIPPOCAMPAL subset by this SAME arc coordinate\n",
    "    H_ok_idx = H_bridge_idx[ok]\n",
    "    H_ok_ringpos = j[ok]\n",
    "    # keep only those that fall inside the final arc\n",
    "    in_arc = np.isin(H_ok_ringpos, arc_pos_final)\n",
    "    H_sel = H_ok_idx[in_arc]\n",
    "    H_sel_ringpos = H_ok_ringpos[in_arc]\n",
    "    if H_sel.size < 2:\n",
    "        raise RuntimeError(\"Not enough hippocampal bridgeheads on selected cortical arc.\")\n",
    "\n",
    "    # map ring pos -> arc index [0..len(arc)-1], then sort H by arc index\n",
    "    pos_to_arc = {int(p): k for k, p in enumerate(arc_pos_final)}\n",
    "    H_arc_idx = np.array([pos_to_arc[int(p)] for p in H_sel_ringpos], dtype=int)\n",
    "    order = np.argsort(H_arc_idx)\n",
    "    H_chain = H_sel[order]  # now H[0] aligns with C[0], H[-1] with C[-1]\n",
    "\n",
    "    # 5) DP seam triangles (tagged)\n",
    "    seam_tris = _dp_seam_triangles(H_V, C_V, H_chain, C_chain)\n",
    "\n",
    "    # 6) Merge into one mesh\n",
    "    nC = len(C_V)\n",
    "    V_out = np.vstack([C_V, H_V]).astype(np.float32, copy=False)\n",
    "    F_c = C_F.astype(np.int64, copy=False)\n",
    "    F_h = (H_F + nC).astype(np.int64, copy=False)\n",
    "\n",
    "    def _abs_idx(tag, idx): return idx + nC if tag == 'H' else idx\n",
    "    if seam_tris:\n",
    "        F_bridge = np.array(\n",
    "            [[_abs_idx(t0,i0), _abs_idx(t1,i1), _abs_idx(t2,i2)]\n",
    "             for ((t0,i0),(t1,i1),(t2,i2)) in seam_tris],\n",
    "            dtype=np.int64\n",
    "        )\n",
    "        # prune degenerates\n",
    "        a = V_out[F_bridge[:,0]]; b = V_out[F_bridge[:,1]]; c = V_out[F_bridge[:,2]]\n",
    "        areas = np.linalg.norm(np.cross(b-a, c-a), axis=1) * 0.5\n",
    "        F_bridge = F_bridge[areas > 1e-12]\n",
    "    else:\n",
    "        F_bridge = np.zeros((0,3), dtype=np.int64)\n",
    "\n",
    "    F_out = np.vstack([F_c, F_h, F_bridge]).astype(np.int32, copy=False)\n",
    "\n",
    "    # Build Gifti\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "    gi_out = GiftiImage()\n",
    "    gi_out.add_gifti_data_array(GiftiDataArray(data=V_out, intent=POINTSET))\n",
    "    gi_out.add_gifti_data_array(GiftiDataArray(data=F_out, intent=TRIANGLE))\n",
    "    return gi_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_surf = stitch_hippocampus_neocortex_gifti_nearestDP(hipp_o, white_nohipp, bridgehead)\n",
    "nib.save(stitched_surf, \"refsurfs/avg_L_stitched_white.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from dataclasses import dataclass\n",
    "from scipy.spatial import cKDTree\n",
    "from nibabel.gifti import GiftiImage, GiftiDataArray\n",
    "from nibabel.nifti1 import intent_codes\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _gifti_vertices_faces(gi: GiftiImage):\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "    vs = [da for da in gi.darrays if da.intent == POINTSET]\n",
    "    fs = [da for da in gi.darrays if da.intent == TRIANGLE]\n",
    "    if len(vs) != 1 or len(fs) != 1:\n",
    "        raise ValueError(\"GiftiImage must contain exactly one POINTSET and one TRIANGLE.\")\n",
    "    V = np.asarray(vs[0].data)\n",
    "    F = np.asarray(fs[0].data, dtype=np.int64)\n",
    "    return V, F, vs[0], fs[0]\n",
    "\n",
    "# ---------- template ----------\n",
    "@dataclass\n",
    "class OverlapStitchTemplate:\n",
    "    n_cortex: int\n",
    "    n_hippo: int\n",
    "    keep_cortex_idx: np.ndarray   # sorted ascending\n",
    "    keep_hippo_idx: np.ndarray    # sorted ascending\n",
    "    faces_template: np.ndarray    # faces reindexed to [C_keep; H_keep] layout (int32)\n",
    "\n",
    "def make_overlap_stitch_template(\n",
    "    ref_cortex_gii: GiftiImage,\n",
    "    ref_hippo_gii: GiftiImage,\n",
    "    ref_stitched_gii: GiftiImage,\n",
    "    tol_mm: float = 1e-4,\n",
    ") -> OverlapStitchTemplate:\n",
    "    \"\"\"\n",
    "    Build a stitch template by matching stitched vertices to either cortex or hippo\n",
    "    vertices via nearest-neighbour overlap (within tol_mm). Produces:\n",
    "      - exact keep indices on cortex/hippo\n",
    "      - stitched faces reindexed to a canonical [C_keep; H_keep] layout\n",
    "    \"\"\"\n",
    "    C_V, _, C_Vda, C_Fda = _gifti_vertices_faces(ref_cortex_gii)\n",
    "    H_V, _, _, _          = _gifti_vertices_faces(ref_hippo_gii)\n",
    "    S_V, S_F, _, _        = _gifti_vertices_faces(ref_stitched_gii)\n",
    "\n",
    "    nC, nH = len(C_V), len(H_V)\n",
    "\n",
    "    # KD-trees on originals\n",
    "    treeC = cKDTree(C_V)\n",
    "    treeH = cKDTree(H_V)\n",
    "    dC, iC = treeC.query(S_V, k=1)\n",
    "    dH, iH = treeH.query(S_V, k=1)\n",
    "\n",
    "    # assign each stitched vertex to its closer source if within tol\n",
    "    src = np.where(dC <= dH, 0, 1)  # 0=cortex, 1=hippo\n",
    "    dmin = np.where(src==0, dC, dH)\n",
    "    idx  = np.where(src==0, iC, iH)\n",
    "\n",
    "    # sanity: all stitched verts must match one side closely\n",
    "    bad = dmin > float(tol_mm)\n",
    "    if np.any(bad):\n",
    "        raise RuntimeError(\n",
    "            f\"{bad.sum()} stitched vertices didn't match cortex/hippo within tol={tol_mm} mm. \"\n",
    "            \"Increase tol_mm slightly or check that stitched verts come from the two sources only.\"\n",
    "        )\n",
    "\n",
    "    keepC = np.unique(idx[src==0])\n",
    "    keepH = np.unique(idx[src==1])\n",
    "    keepC_sorted = np.sort(keepC)\n",
    "    keepH_sorted = np.sort(keepH)\n",
    "\n",
    "    # map original -> new index in [C_keep; H_keep]\n",
    "    mapC = -np.ones(nC, dtype=np.int64)\n",
    "    mapH = -np.ones(nH, dtype=np.int64)\n",
    "    mapC[keepC_sorted] = np.arange(len(keepC_sorted), dtype=np.int64)\n",
    "    mapH[keepH_sorted] = np.arange(len(keepH_sorted), dtype=np.int64)\n",
    "\n",
    "    # map stitched vertex -> new index\n",
    "    new_idx = np.empty(len(S_V), dtype=np.int64)\n",
    "    isC = (src == 0)\n",
    "    new_idx[isC]  = mapC[idx[isC]]\n",
    "    new_idx[~isC] = len(keepC_sorted) + mapH[idx[~isC]]\n",
    "\n",
    "    # reindex faces to the canonical [C_keep; H_keep]\n",
    "    F_template = new_idx[S_F]\n",
    "    F_template = F_template.astype(np.int32, copy=False)\n",
    "\n",
    "    return OverlapStitchTemplate(\n",
    "        n_cortex=nC,\n",
    "        n_hippo=nH,\n",
    "        keep_cortex_idx=keepC_sorted.astype(np.int64, copy=False),\n",
    "        keep_hippo_idx=keepH_sorted.astype(np.int64, copy=False),\n",
    "        faces_template=F_template,\n",
    "    )\n",
    "\n",
    "def apply_overlap_stitch_template(\n",
    "    cortex_gii: GiftiImage,\n",
    "    hippo_gii: GiftiImage,\n",
    "    tmpl: OverlapStitchTemplate,\n",
    "    preserve_metadata_from: GiftiImage = None,\n",
    ") -> GiftiImage:\n",
    "    \"\"\"\n",
    "    Apply an OverlapStitchTemplate to new cortex/hippo meshes that share the same\n",
    "    vertex correspondence (same n_cortex/n_hippo and indexing as the reference).\n",
    "\n",
    "    Returns a stitched GiftiImage with:\n",
    "      vertices = [ cortex[keep_cortex_idx] ; hippo[keep_hippo_idx] ]\n",
    "      faces    = tmpl.faces_template\n",
    "    \"\"\"\n",
    "    C_V, _, C_Vda, C_Fda = _gifti_vertices_faces(cortex_gii)\n",
    "    H_V, _, H_Vda, H_Fda = _gifti_vertices_faces(hippo_gii)\n",
    "\n",
    "    if len(C_V) != tmpl.n_cortex or len(H_V) != tmpl.n_hippo:\n",
    "        raise ValueError(\n",
    "            f\"New meshes do not match template counts \"\n",
    "            f\"(got C={len(C_V)}/H={len(H_V)}, expected C={tmpl.n_cortex}/H={tmpl.n_hippo}).\"\n",
    "        )\n",
    "\n",
    "    V_out = np.vstack([\n",
    "        C_V[tmpl.keep_cortex_idx],\n",
    "        H_V[tmpl.keep_hippo_idx],\n",
    "    ]).astype(np.float32, copy=False)\n",
    "    F_out = tmpl.faces_template.astype(np.int32, copy=False)\n",
    "\n",
    "    # choose metadata source\n",
    "    src = preserve_metadata_from if preserve_metadata_from is not None else cortex_gii\n",
    "    _, _, V_da_src, F_da_src = _gifti_vertices_faces(src)\n",
    "\n",
    "    POINTSET = intent_codes['NIFTI_INTENT_POINTSET']\n",
    "    TRIANGLE = intent_codes['NIFTI_INTENT_TRIANGLE']\n",
    "    gi = GiftiImage()\n",
    "    gi.add_gifti_data_array(GiftiDataArray(\n",
    "        data=V_out, intent=POINTSET,\n",
    "        datatype=V_da_src.datatype, encoding=V_da_src.encoding,\n",
    "        endian=V_da_src.endian, coordsys=V_da_src.coordsys, meta=V_da_src.meta\n",
    "    ))\n",
    "    gi.add_gifti_data_array(GiftiDataArray(\n",
    "        data=F_out, intent=TRIANGLE,\n",
    "        datatype=F_da_src.datatype, encoding=F_da_src.encoding,\n",
    "        endian=F_da_src.endian, coordsys=F_da_src.coordsys, meta=F_da_src.meta\n",
    "    ))\n",
    "    return gi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = make_overlap_stitch_template(white, hipp_o, stitched_surf)\n",
    "new_stitched = apply_overlap_stitch_template(pial, hipp_i, template)\n",
    "nib.save(new_stitched, \"refsurfs/avg_L_stitched_pial.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = make_overlap_stitch_template(white, hipp_o, stitched_surf)\n",
    "new_stitched = apply_overlap_stitch_template(pial, hipp_i, template)\n",
    "nib.save(new_stitched, \"refsurfs/avg_L_stitched_pial.surf.gii\")\n",
    "\n",
    "hipp_o = nib.load(\"refsurfs/avg_R_hu_outer.surf.gii\")\n",
    "white = nib.load(\"refsurfs/avg_R_fs_white.surf.gii\")\n",
    "new_stitched_white = apply_overlap_stitch_template(white, hipp_o, template)\n",
    "nib.save(new_stitched_white, \"refsurfs/avg_R_stitched_white.surf.gii\")\n",
    "\n",
    "hipp_i = nib.load(\"refsurfs/avg_R_hu_inner.surf.gii\")\n",
    "pial = nib.load(\"refsurfs/avg_R_fs_pial.surf.gii\")\n",
    "new_stitched_pial = apply_overlap_stitch_template(pial, hipp_i, template)\n",
    "nib.save(new_stitched_pial, \"refsurfs/avg_R_stitched_pial.surf.gii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hippunfold-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
